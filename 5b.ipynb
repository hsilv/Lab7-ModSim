{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar, tendremos que para utilizar estas funciones, se necesita la función objetivo $f$, su gradiente $df$, y el hessiano $ddf$. Definiendo la estructura genérica para los métodos de descenso:\n",
    "\n",
    "* `f`: Función objetivo\n",
    "* `df`: Gradiente de la función objetivo.\n",
    "* `ddf`: El hessiano de la función objetivo.\n",
    "* `x0`: Punto de inicio.\n",
    "* `a`: Step size.\n",
    "* `maxIter`: Número máximo de pasos o iteraciones.\n",
    "* `e`: Tolerancia para el criterio de paro.\n",
    "* `stopCriterion`: Criterio de paro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Descenso Gradiente Naive con Dirección Aleatoria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradNaiveRand (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function GradNaiveRand(f, df, x0, a, maxIter, e, stopCriterion)\n",
    "    # Definir el punto inicial\n",
    "    x = x0\n",
    "\n",
    "    # x_k es la secuencia de puntos x obtenidos en cada iteración\n",
    "    x_k = [x0]\n",
    "\n",
    "    # f_k es la secuencia de valores de la función objetivo en cada iteración\n",
    "    f_k = [f(x0)]\n",
    "    errors = []\n",
    "\n",
    "    for k in 1:maxIter\n",
    "        # Generar la dirección de descenso al gradiente de forma aleatoria\n",
    "        d = randn(length(x))\n",
    "\n",
    "        # Normalizar la dirección de descenso\n",
    "        d = d/norm(d)\n",
    "\n",
    "        # Calcular el nuevo punto\n",
    "        x_new = x + a*d\n",
    "\n",
    "        # Guardar el nuevo punto y el valor de la función objetivo\n",
    "        push!(x_k, x_new)\n",
    "        push!(f_k, f(x_new))\n",
    "\n",
    "        # Calcular el error\n",
    "        error = norm(df(x_new))\n",
    "\n",
    "        # Guardar el error\n",
    "        push!(errors, error)\n",
    "\n",
    "        # Si se cumple el criterio de paro, terminar\n",
    "        if error < e\n",
    "            return x_new, x_k, f_k, errors, k, true\n",
    "        end\n",
    "\n",
    "        # Actualizar el punto actual\n",
    "        x = x_new    \n",
    "    end\n",
    "\n",
    "    # Retornar el punto actual, la secuencia de puntos, la secuencia de valores de la función objetivo, la secuencia de errores, el número de iteraciones\n",
    "    # Y un indicador que no converge\n",
    "    return x, x_k, f_k, errors, maxIter, false\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Descenso Máximo Naive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradNaiveMax (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function GradNaiveMax(f, df, x0, a, maxIter, e, stopCriterion)\n",
    "    # Definir el punto inicial\n",
    "    x = x0\n",
    "\n",
    "    # x_k es la secuencia de puntos x obtenidos en cada iteración\n",
    "    x_k = [x0]\n",
    "\n",
    "    # f_k es la secuencia de valores de la función objetivo en cada iteración\n",
    "    f_k = [f(x0)]\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    for k in 1:maxIter\n",
    "        # Calcular la dirección de descenso gradiente\n",
    "        d = df(x)\n",
    "\n",
    "        # Asegurarse de que x y d tengan las mismas dimensiones\n",
    "        if length(x) != length(d)\n",
    "            throw(DimensionMismatch(\"x y d deben tener las mismas dimensiones\"))\n",
    "        end\n",
    "\n",
    "        # Calcular el nuevo punto\n",
    "        x_new = x + a*d\n",
    "\n",
    "        # Guardar el nuevo punto y el valor de la función objetivo\n",
    "        push!(x_k, x_new)\n",
    "        push!(f_k, f(x_new))\n",
    "\n",
    "        # Calcular el error\n",
    "        error = norm(df(x))\n",
    "\n",
    "        # Guardar el error\n",
    "        push!(errors, error)\n",
    "\n",
    "        # Si se cumple el criterio de paro, terminar\n",
    "        if error < e\n",
    "            return x_new, x_k, f_k, errors, k, true\n",
    "        end\n",
    "\n",
    "        # Actualizar el punto actual\n",
    "        x = x_new\n",
    "    end\n",
    "\n",
    "    # Retornar el punto actual, la secuencia de puntos, la secuencia de valores de la función objetivo, la secuencia de errores, el número de iteraciones\n",
    "    # Y un indicador que no converge\n",
    "    return x, x_k, f_k, errors, maxIter, false\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Descenso Gradiente de Newton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradNewton (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function GradNewton(f, df, x0, a, maxIter, e, stopCriterion)\n",
    "    # Definir el punto inicial\n",
    "    x = x0\n",
    "\n",
    "    # x_k es la secuencia de puntos x obtenidos en cada iteración\n",
    "    x_k = [x0]\n",
    "\n",
    "    # f_k es la secuencia de valores de la función objetivo en cada iteración\n",
    "    f_k = [f(x0)]\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    # Inicializar la matriz identidad para el Hessiano\n",
    "    H_I = I(length(x0))\n",
    "\n",
    "    for k in 1:maxIter\n",
    "        # Calcular la dirección de descenso\n",
    "        d = - H_I * df(x)\n",
    "\n",
    "        # Calcular el nuevo punto\n",
    "        x_new = x + a*d\n",
    "\n",
    "        # Guardar el nuevo punto y el valor de la función objetivo\n",
    "        push!(x_k, x_new)\n",
    "        push!(f_k, f(x_new))\n",
    "\n",
    "        # Calcular el error\n",
    "        error = norm(df(x))\n",
    "        push!(errors, error)\n",
    "\n",
    "        # Si se cumple el criterio de paro, terminar\n",
    "        if error < e\n",
    "            return x_new, x_k, f_k, errors, k, true\n",
    "        end\n",
    "\n",
    "        # Actualización del Hessiano Inverso\n",
    "        s = x_new - x\n",
    "        y = df(x_new) - df(x)\n",
    "        rho = 1/(y'*s)\n",
    "        H_I = (I -rho*s*y')*H_I*(I-rho*y*s') + rho*s*s'\n",
    "\n",
    "        # Actualizar el punto actual\n",
    "        x = x_new\n",
    "\n",
    "    end\n",
    "\n",
    "    return x, x_k, f_k, errors, maxIter, false\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Descenso Gradiente de Newton (Hessiano Exacto)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradNewtonExact (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function GradNewtonExact(f, df, ddf, x0, a, maxIter, e, stopCriterion)\n",
    "\n",
    "    λ = 1e-6  # Regularización\n",
    "\n",
    "    # Definir el punto inicial\n",
    "    x = x0\n",
    "\n",
    "    # x_k es la secuencia de puntos x obtenidos en cada iteración\n",
    "    x_k = [x0]\n",
    "\n",
    "    # f_k es la secuencia de valores de la función objetivo en cada iteración\n",
    "    f_k = [f(x0)]\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    for k in 1:maxIter\n",
    "        # Obtener la dirección de descenso\n",
    "        g = df(x)\n",
    "        H = ddf(x) + λ * I\n",
    "\n",
    "        # Resolver el sistema de ecuaciones\n",
    "        d = -H\\g\n",
    "\n",
    "        # Calcular el nuevo punto\n",
    "        x_new = x + a*d\n",
    "\n",
    "        # Guardar el nuevo punto y el valor de la función objetivo\n",
    "        push!(x_k, x_new)\n",
    "        push!(f_k, f(x_new))\n",
    "\n",
    "        # Calcular el error\n",
    "        error = norm(df(x))\n",
    "        push!(errors, error)\n",
    "\n",
    "        # Si se cumple el criterio de paro, terminar\n",
    "        if error < e\n",
    "            return x_new, x_k, f_k, errors, k, true\n",
    "        end\n",
    "\n",
    "        # Actualizar el punto actual\n",
    "        x = x_new\n",
    "    end\n",
    "\n",
    "    return x, x_k, f_k, errors, maxIter, false\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testeando la función $ f : \\R^{2} \\to \\R $ dada por**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x_1,x_2) = 100(x_2-x_{1}^{2})^{2}+(1-x_{1})^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function f(x)\n",
    "    x1, x2 = x\n",
    "    return 100*(x2 - x1^2)^2 + (1 - x1)^2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradiente de la función**:\n",
    "\n",
    "El gradiente $\\nabla f(x_1,x_2)$\n",
    "\n",
    "$$ \\nabla f(x_1, x_2) = \\frac{\\partial f}{\\partial x_1} = -400x_1(x_2 - x_{1}^{2})-2(1-x_1), \\frac{\\partial f}{\\partial x_2} = 200(x_2 - x_{1}^{2})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function df(x)\n",
    "    x1, x2 = x\n",
    "    df_dx1 = -400 * x1 * (x2 - x1^2) - 2 * (1 - x1)\n",
    "    df_dx2 = 200 * (x2 - x1^2)\n",
    "    return [df_dx1, df_dx2]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hessiano de la función**:\n",
    "\n",
    "El hessiano $H(f(x_1,x_2))$\n",
    "\n",
    "$$ H(f) = \n",
    "\\begin{bmatrix}\n",
    "1200 * x_{1}^{2} - 400 * x_2 + 2 & -400 * x_1\\\\\n",
    "-400 * x_1 & 200\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ddf (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function ddf(x)\n",
    "    x1, x2 = x\n",
    "    h11 = 1200 * x1^2 - 400 * x2 + 2\n",
    "    h12 = -400 * x1\n",
    "    h21 = -400 * x1\n",
    "    h22 = 200\n",
    "    return [h11 h12; h21 h22]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Punto inicial**: $x_0 = (-1.2, 1)^{T}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -1.2\n",
       " -1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x0 = [-1.2, -1.0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":gradiente"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "α = 0.001  # Tamaño de paso\n",
    "maxIter = 10000  # Máximo de iteraciones\n",
    "ε = 1e-6  # Tolerancia\n",
    "stop_criterion = :gradiente  # Criterio de paro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de Descenso Gradiente Naive con Dirección Aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solución óptima: [-1.2660623299215223, -0.3660746384699944]\n",
      "Número de iteraciones: 1000000\n",
      "Valor de f en la solución: 392.8265947202847\n",
      "Convergencia: false\n"
     ]
    }
   ],
   "source": [
    "α = 0.001  # Tamaño de paso\n",
    "maxIter = 1000000\n",
    "x_opt, xk, fk, errores, iteraciones, convergencia = GradNaiveRand(f, df, x0, α, maxIter, ε, stop_criterion)\n",
    "println(\"Solución óptima: \", x_opt)\n",
    "println(\"Número de iteraciones: \", iteraciones)\n",
    "println(\"Valor de f en la solución: \", f(x_opt))\n",
    "println(\"Convergencia: \", convergencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de Descenso Máximo Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solución óptima: [-Inf, -Inf]\n",
      "Número de iteraciones: 1000000\n",
      "Valor de f en la solución: Inf\n",
      "Convergencia: false\n"
     ]
    }
   ],
   "source": [
    "α = 0.00001  # Tamaño de paso\n",
    "maxIter = 1000000\n",
    "x_opt, xk, fk, errores, iteraciones, convergencia = GradNaiveMax(f, df, x0, α, maxIter, ε, stop_criterion)\n",
    "println(\"Solución óptima: \", x_opt)\n",
    "println(\"Número de iteraciones: \", iteraciones)\n",
    "println(\"Valor de f en la solución: \", f(x_opt))\n",
    "println(\"Convergencia: \", convergencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de Descenso Gradiente de Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solución óptima: [0.9999991158655859, 0.9999982295635587]\n",
      "Número de iteraciones: 207384\n",
      "Valor de f en la solución: 7.82163855840224e-13\n",
      "Convergencia: true\n"
     ]
    }
   ],
   "source": [
    "α = 0.0001\n",
    "maxIter = 1000000\n",
    "x_opt, xk, fk, errores, iteraciones, convergencia = GradNewton(f, df, x0, α, maxIter, ε, stop_criterion)\n",
    "println(\"Solución óptima: \", x_opt)\n",
    "println(\"Número de iteraciones: \", iteraciones)\n",
    "println(\"Valor de f en la solución: \", f(x_opt))\n",
    "println(\"Convergencia: \", convergencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de Descenso Gradiente de Newton (Hessiano Exacto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solución óptima: [0.9999991434244574, 0.9999982848600448]\n",
      "Número de iteraciones: 209583\n",
      "Valor de f en la solución: 7.341175124825433e-13\n",
      "Convergencia: true\n"
     ]
    }
   ],
   "source": [
    "x_opt, xk, fk, errores, iteraciones, convergencia = GradNewtonExact(f, df, ddf, x0, α, maxIter, ε, stop_criterion)\n",
    "println(\"Solución óptima: \", x_opt)\n",
    "println(\"Número de iteraciones: \", iteraciones)\n",
    "println(\"Valor de f en la solución: \", f(x_opt))\n",
    "println(\"Convergencia: \", convergencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Óptimo**: $x^{*} = (1, 1)^{T}, \\ \\ \\ f(x^{*}) = 0$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
