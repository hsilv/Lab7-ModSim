{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar, tendremos que para utilizar estas funciones, se necesita la función objetivo $f$, su gradiente $df$, y el hessiano $ddf$. Definiendo la estructura genérica para los métodos de descenso:\n",
    "\n",
    "* `f`: Función objetivo\n",
    "* `df`: Gradiente de la función objetivo.\n",
    "* `ddf`: El hessiano de la función objetivo.\n",
    "* `x0`: Punto de inicio.\n",
    "* `a`: Step size.\n",
    "* `maxIter`: Número máximo de pasos o iteraciones.\n",
    "* `e`: Tolerancia para el criterio de paro.\n",
    "* `stopCriterion`: Criterio de paro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Descenso Gradiente Naive con Dirección Aleatoria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradNaiveRand (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function GradNaiveRand(f, df, x0, a, maxIter, e, stopCriterion)\n",
    "    # Definir el punto inicial\n",
    "    x = x0\n",
    "\n",
    "    # x_k es la secuencia de puntos x obtenidos en cada iteración\n",
    "    x_k = [x0]\n",
    "\n",
    "    # f_k es la secuencia de valores de la función objetivo en cada iteración\n",
    "    f_k = [f(x0)]\n",
    "    errors = []\n",
    "\n",
    "    for k in 1:maxIter\n",
    "        # Generar la dirección de descenso al gradiente de forma aleatoria\n",
    "        d = randn(length(x))\n",
    "\n",
    "        # Normalizar la dirección de descenso\n",
    "        d = d/norm(d)\n",
    "\n",
    "        # Calcular el nuevo punto\n",
    "        x_new = x + a*d\n",
    "\n",
    "        # Guardar el nuevo punto y el valor de la función objetivo\n",
    "        push!(x_k, x_new)\n",
    "        push!(f_k, f(x_new))\n",
    "\n",
    "        # Calcular el error\n",
    "        error = norm(df(x_new))\n",
    "\n",
    "        # Guardar el error\n",
    "        push!(errors, error)\n",
    "\n",
    "        # Si se cumple el criterio de paro, terminar\n",
    "        if error < e\n",
    "            return x_new, x_k, f_k, errors, k, true\n",
    "        end\n",
    "\n",
    "        # Actualizar el punto actual\n",
    "        x = x_new    \n",
    "    end\n",
    "\n",
    "    # Retornar el punto actual, la secuencia de puntos, la secuencia de valores de la función objetivo, la secuencia de errores, el número de iteraciones\n",
    "    # Y un indicador que no converge\n",
    "    return x, x_k, f_k, errors, maxIter, false\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Descenso Máximo Naive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradNaiveMax (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function GradNaiveMax(f, df, x0, a, maxIter, e, stopCriterion)\n",
    "    # Definir el punto inicial\n",
    "    x = x0\n",
    "\n",
    "    # x_k es la secuencia de puntos x obtenidos en cada iteración\n",
    "    x_k = [x0]\n",
    "\n",
    "    # f_k es la secuencia de valores de la función objetivo en cada iteración\n",
    "    f_k = [f(x0)]\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    for k in 1:maxIter\n",
    "        # Calcular la dirección de descenso gradiente\n",
    "        d = df(x)\n",
    "\n",
    "        # Asegurarse de que x y d tengan las mismas dimensiones\n",
    "        if length(x) != length(d)\n",
    "            throw(DimensionMismatch(\"x y d deben tener las mismas dimensiones\"))\n",
    "        end\n",
    "\n",
    "        # Calcular el nuevo punto\n",
    "        x_new = x - a*d  # Cambiado a restar el gradiente\n",
    "\n",
    "        # Guardar el nuevo punto y el valor de la función objetivo\n",
    "        push!(x_k, x_new)\n",
    "        push!(f_k, f(x_new))\n",
    "\n",
    "        # Calcular el error\n",
    "        error = norm(df(x_new))  # Cambiado a x_new\n",
    "\n",
    "        # Guardar el error\n",
    "        push!(errors, error)\n",
    "\n",
    "        # Si se cumple el criterio de paro, terminar\n",
    "        if error < e\n",
    "            return x_new, x_k, f_k, errors, k, true\n",
    "        end\n",
    "\n",
    "        # Actualizar el punto actual\n",
    "        x = x_new\n",
    "    end\n",
    "\n",
    "    # Retornar el punto actual, la secuencia de puntos, la secuencia de valores de la función objetivo, la secuencia de errores, el número de iteraciones\n",
    "    # Y un indicador que no converge\n",
    "    return x, x_k, f_k, errors, maxIter, false\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Descenso Gradiente de Newton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradNewton (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function GradNewton(f, df, x0, a, maxIter, e, stopCriterion)\n",
    "    # Definir el punto inicial\n",
    "    x = x0\n",
    "\n",
    "    # x_k es la secuencia de puntos x obtenidos en cada iteración\n",
    "    x_k = [x0]\n",
    "\n",
    "    # f_k es la secuencia de valores de la función objetivo en cada iteración\n",
    "    f_k = [f(x0)]\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    # Inicializar la matriz identidad para el Hessiano\n",
    "    H_I = I(length(x0))\n",
    "\n",
    "    for k in 1:maxIter\n",
    "        # Calcular la dirección de descenso\n",
    "        d = - H_I * df(x)\n",
    "\n",
    "        # Calcular el nuevo punto\n",
    "        x_new = x + a*d\n",
    "\n",
    "        # Guardar el nuevo punto y el valor de la función objetivo\n",
    "        push!(x_k, x_new)\n",
    "        push!(f_k, f(x_new))\n",
    "\n",
    "        # Calcular el error\n",
    "        error = norm(df(x))\n",
    "        push!(errors, error)\n",
    "\n",
    "        # Si se cumple el criterio de paro, terminar\n",
    "        if error < e\n",
    "            return x_new, x_k, f_k, errors, k, true\n",
    "        end\n",
    "\n",
    "        # Actualización del Hessiano Inverso\n",
    "        s = x_new - x\n",
    "        y = df(x_new) - df(x)\n",
    "        rho = 1/(y'*s)\n",
    "        H_I = (I -rho*s*y')*H_I*(I-rho*y*s') + rho*s*s'\n",
    "\n",
    "        # Actualizar el punto actual\n",
    "        x = x_new\n",
    "\n",
    "    end\n",
    "\n",
    "    return x, x_k, f_k, errors, maxIter, false\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Descenso Gradiente de Newton (Hessiano Exacto)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradNewtonExact (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function GradNewtonExact(f, df, ddf, x0, a, maxIter, e, stopCriterion)\n",
    "\n",
    "    λ = 1e-6  # Regularización\n",
    "\n",
    "    # Definir el punto inicial\n",
    "    x = x0\n",
    "\n",
    "    # x_k es la secuencia de puntos x obtenidos en cada iteración\n",
    "    x_k = [x0]\n",
    "\n",
    "    # f_k es la secuencia de valores de la función objetivo en cada iteración\n",
    "    f_k = [f(x0)]\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    for k in 1:maxIter\n",
    "        # Obtener la dirección de descenso\n",
    "        g = df(x)\n",
    "        H = ddf(x) + λ * I\n",
    "\n",
    "        # Resolver el sistema de ecuaciones\n",
    "        d = -H\\g\n",
    "\n",
    "        # Calcular el nuevo punto\n",
    "        x_new = x + a*d\n",
    "\n",
    "        # Guardar el nuevo punto y el valor de la función objetivo\n",
    "        push!(x_k, x_new)\n",
    "        push!(f_k, f(x_new))\n",
    "\n",
    "        # Calcular el error\n",
    "        error = norm(df(x))\n",
    "        push!(errors, error)\n",
    "\n",
    "        # Si se cumple el criterio de paro, terminar\n",
    "        if error < e\n",
    "            return x_new, x_k, f_k, errors, k, true\n",
    "        end\n",
    "\n",
    "        # Actualizar el punto actual\n",
    "        x = x_new\n",
    "    end\n",
    "\n",
    "    return x, x_k, f_k, errors, maxIter, false\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testeando la función $ f : \\R^{10} \\to \\R $ dada por**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = \\sum_{i=1}^{9} [100(x_{i+1} - x_{i}^{2})^{2} + (1-x_i)^2]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function f(x)\n",
    "    n = length(x)\n",
    "    fu = 0.0\n",
    "    for i in 1:n-1\n",
    "        fu += 100 * (x[i+1] - x[i]^2)^2 + (1 - x[i])^2\n",
    "    end\n",
    "    return fu\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradiente de la función**:\n",
    "\n",
    "El gradiente $\\nabla f(x_i)$\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x_i} =\n",
    "\\begin{cases}\n",
    "-400x_i(x_{i+1} - x_{i}^{2})-2(1-x_i), & \\text{ si} \\leq i \\lt n\\\\\n",
    "200(x_i - x_{i-1}^{2}), & \\text{ si} \\ \\ i = n\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function df(x)\n",
    "    n = length(x)\n",
    "    grad = zeros(n)\n",
    "    for i in 1:n-1\n",
    "        grad[i] = -400 * x[i] * (x[i+1] - x[i]^2) - 2 * (1 - x[i])\n",
    "    end\n",
    "    grad[n] = 200 * (x[n] - x[n-1]^2)\n",
    "    return grad\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hessiano de la función**:\n",
    "\n",
    "El hessiano $H(f(x_1,x_2))$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial ^2 f}{\\partial x_{i}^{2}} = 1200x_{i}^{2} - 400x_{i+1} + 2, \\ \\ \\ \\ \\ 1 \\leq i \\lt n \\\\\n",
    "\n",
    "\\frac{\\partial ^2 f}{\\partial x_{i} \\partial x_{i+1}} = -400x_i, \\ \\ \\ \\ \\ \\text{ y su simétrico}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ddf (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function ddf(x)\n",
    "    n = length(x)\n",
    "    H = zeros(n, n)\n",
    "    for i in 1:n-1\n",
    "        H[i,i] = 1200 * x[i]^2 - 400 * x[i+1] + 2\n",
    "        H[i,i+1] = -400 * x[i]\n",
    "        H[i+1,i] = H[i,i+1]  # Simétrico\n",
    "    end\n",
    "    H[n,n] = 200\n",
    "    return H\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Punto inicial**: $x_0 = (-1.2, 1, 1, . . . , 1, -1.2, 1)^{T}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11-element Vector{Float64}:\n",
       " -1.2\n",
       "  1.0\n",
       "  1.0\n",
       "  1.0\n",
       "  1.0\n",
       "  1.0\n",
       "  1.0\n",
       "  1.0\n",
       "  1.0\n",
       " -1.2\n",
       "  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x0 = [-1.2; ones(8); -1.2; 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":gradiente"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "α = 0.001  # Tamaño de paso\n",
    "maxIter = 10000  # Máximo de iteraciones\n",
    "ε = 1e-6  # Tolerancia\n",
    "stop_criterion = :gradiente  # Criterio de paro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de Descenso Gradiente Naive con Dirección Aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solución óptima: [-1.2042246762984767, 1.053117798955782, 1.0353653774198246, 0.9712467115148897, 1.0222928294449913, 0.9905055202231092, 1.0101394276023004, 1.013853609184866, 0.9988808303588836, -1.1990077649564344, 0.9352705427169383]\n",
      "Número de iteraciones: 10000\n",
      "Valor de f en la solución: 535.9317627056361\n",
      "Convergencia: false\n"
     ]
    }
   ],
   "source": [
    "α = 0.001  # Tamaño de paso\n",
    "maxIter = 10000\n",
    "x_opt, xk, fk, errores, iteraciones, convergencia = GradNaiveRand(f, df, x0, α, maxIter, ε, stop_criterion)\n",
    "println(\"Solución óptima: \", x_opt)\n",
    "println(\"Número de iteraciones: \", iteraciones)\n",
    "println(\"Valor de f en la solución: \", f(x_opt))\n",
    "println(\"Convergencia: \", convergencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de Descenso Máximo Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plot (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solución óptima: [-0.9949747447323938, 0.999999995837412, 0.9999999916581671, 0.9999999832829538, 0.9999999664990131, 0.9999999328639697, 0.9999998654592923, 0.9999997303802246, 0.9999994596816122, 0.9999989172013792, 0.9999978300710032]\n",
      "Número de iteraciones: 321671\n",
      "Valor de f en la solución: 3.9899748022582036\n",
      "Convergencia: true\n"
     ]
    }
   ],
   "source": [
    "α = 0.0001\n",
    "maxIter = 1000000\n",
    "x_opt, xk, fk, errores, iteraciones, convergencia = GradNaiveMax(f, df, x0, α, maxIter, ε, stop_criterion)\n",
    "graficar_error_algoritmo(errores, \"Gradiente Aleatorio\")\n",
    "println(\"Solución óptima: \", x_opt)\n",
    "println(\"Número de iteraciones: \", iteraciones)\n",
    "println(\"Valor de f en la solución: \", f(x_opt))\n",
    "println(\"Convergencia: \", convergencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de Descenso Gradiente de Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solución óptima: [-0.9949747458908894, 0.9999999975662875, 0.9999999951197805, 0.9999999902153807, 0.9999999803781668, 0.9999999606493017, 0.9999999210747166, 0.9999998417044168, 0.9999996825673902, 0.9999993615804497, 0.9999987204945936]\n",
      "Número de iteraciones: 2341126\n",
      "Valor de f en la solución: 3.989974805723667\n",
      "Convergencia: true\n"
     ]
    }
   ],
   "source": [
    "α = 0.00001\n",
    "maxIter = 4000000\n",
    "x_opt, xk, fk, errores, iteraciones, convergencia = GradNewton(f, df, x0, α, maxIter, ε, stop_criterion)\n",
    "println(\"Solución óptima: \", x_opt)\n",
    "println(\"Número de iteraciones: \", iteraciones)\n",
    "println(\"Valor de f en la solución: \", f(x_opt))\n",
    "println(\"Convergencia: \", convergencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de Descenso Gradiente de Newton (Hessiano Exacto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solución óptima: [0.07664243495304889, -0.054363947975469486, 0.09989884519040765, -0.03508220253069501, 0.14865467553206307, -0.006542642684043277, 0.7668635501806548, 0.5865591688254453, 0.3494405293481995, 0.13074551445160126, 0.01709438894321838]\n",
      "Número de iteraciones: 6000000\n",
      "Valor de f en la solución: 69.56101700777683\n",
      "Convergencia: false\n"
     ]
    }
   ],
   "source": [
    "maxIter = 6000000\n",
    "x_opt, xk, fk, errores, iteraciones, convergencia = GradNewtonExact(f, df, ddf, x0, α, maxIter, ε, stop_criterion)\n",
    "println(\"Solución óptima: \", x_opt)\n",
    "println(\"Número de iteraciones: \", iteraciones)\n",
    "println(\"Valor de f en la solución: \", f(x_opt))\n",
    "println(\"Convergencia: \", convergencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Óptimo**: $x^{*} = (1, 1, ... , 1)^{T}, \\ \\ \\ f(x^{*}) = 0$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
