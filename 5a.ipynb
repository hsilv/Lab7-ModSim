{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar, tendremos que para utilizar estas funciones, se necesita la función objetivo $f$, su gradiente $df$, y el hessiano $ddf$. Definiendo la estructura genérica para los métodos de descenso:\n",
    "\n",
    "* `f`: Función objetivo\n",
    "* `df`: Gradiente de la función objetivo.\n",
    "* `ddf`: El hessiano de la función objetivo.\n",
    "* `x0`: Punto de inicio.\n",
    "* `a`: Step size.\n",
    "* `maxIter`: Número máximo de pasos o iteraciones.\n",
    "* `e`: Tolerancia para el criterio de paro.\n",
    "* `stopCriterion`: Criterio de paro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Descenso Gradiente Naive con Dirección Aleatoria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradNaiveRand (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function GradNaiveRand(f, df, x0, a, maxIter, e, stopCriterion)\n",
    "    # Definir el punto inicial\n",
    "    x = x0\n",
    "\n",
    "    # x_k es la secuencia de puntos x obtenidos en cada iteración\n",
    "    x_k = [x0]\n",
    "\n",
    "    # f_k es la secuencia de valores de la función objetivo en cada iteración\n",
    "    f_k = [f(x0)]\n",
    "    errors = []\n",
    "\n",
    "    for k in 1:maxIter\n",
    "        # Generar la dirección de descenso al gradiente de forma aleatoria\n",
    "        d = randn(length(x))\n",
    "\n",
    "        # Normalizar la dirección de descenso\n",
    "        d = d/norm(d)\n",
    "\n",
    "        # Calcular el nuevo punto\n",
    "        x_new = x + a*d\n",
    "\n",
    "        # Guardar el nuevo punto y el valor de la función objetivo\n",
    "        push!(x_k, x_new)\n",
    "        push!(f_k, f(x_new))\n",
    "\n",
    "        # Calcular el error\n",
    "        error = norm(df(x_new))\n",
    "\n",
    "        # Guardar el error\n",
    "        push!(errors, error)\n",
    "\n",
    "        # Si se cumple el criterio de paro, terminar\n",
    "        if error < e\n",
    "            return x_new, x_k, f_k, errors, k, true\n",
    "        end\n",
    "\n",
    "        # Actualizar el punto actual\n",
    "        x = x_new    \n",
    "    end\n",
    "\n",
    "    # Retornar el punto actual, la secuencia de puntos, la secuencia de valores de la función objetivo, la secuencia de errores, el número de iteraciones\n",
    "    # Y un indicador que no converge\n",
    "    return x, x_k, f_k, errors, maxIter, false\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Descenso Máximo Naive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradNaiveMax (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function GradNaiveMax(f, df, x0, a, maxIter, e, stopCriterion)\n",
    "    # Definir el punto inicial\n",
    "    x = x0\n",
    "\n",
    "    # x_k es la secuencia de puntos x obtenidos en cada iteración\n",
    "    x_k = [x0]\n",
    "\n",
    "    # f_k es la secuencia de valores de la función objetivo en cada iteración\n",
    "    f_k = [f(x0)]\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    for k in 1:maxIter\n",
    "        # Calcular la dirección de descenso gradiente\n",
    "        d = df(x)\n",
    "\n",
    "        # Asegurarse de que x y d tengan las mismas dimensiones\n",
    "        if length(x) != length(d)\n",
    "            throw(DimensionMismatch(\"x y d deben tener las mismas dimensiones\"))\n",
    "        end\n",
    "\n",
    "        # Calcular el nuevo punto\n",
    "        x_new = x + a*d\n",
    "\n",
    "        # Guardar el nuevo punto y el valor de la función objetivo\n",
    "        push!(x_k, x_new)\n",
    "        push!(f_k, f(x_new))\n",
    "\n",
    "        # Calcular el error\n",
    "        error = norm(df(x))\n",
    "\n",
    "        # Guardar el error\n",
    "        push!(errors, error)\n",
    "\n",
    "        # Si se cumple el criterio de paro, terminar\n",
    "        if error < e\n",
    "            return x_new, x_k, f_k, errors, k, true\n",
    "        end\n",
    "\n",
    "        # Actualizar el punto actual\n",
    "        x = x_new\n",
    "    end\n",
    "\n",
    "    # Retornar el punto actual, la secuencia de puntos, la secuencia de valores de la función objetivo, la secuencia de errores, el número de iteraciones\n",
    "    # Y un indicador que no converge\n",
    "    return x, x_k, f_k, errors, maxIter, false\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Descenso Gradiente de Newton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradNewton (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function GradNewton(f, df, x0, a, maxIter, e, stopCriterion)\n",
    "    # Definir el punto inicial\n",
    "    x = x0\n",
    "\n",
    "    # x_k es la secuencia de puntos x obtenidos en cada iteración\n",
    "    x_k = [x0]\n",
    "\n",
    "    # f_k es la secuencia de valores de la función objetivo en cada iteración\n",
    "    f_k = [f(x0)]\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    # Inicializar la matriz identidad para el Hessiano\n",
    "    H_I = I(length(x0))\n",
    "\n",
    "    for k in 1:maxIter\n",
    "        # Calcular la dirección de descenso\n",
    "        d = - H_I * df(x)\n",
    "\n",
    "        # Calcular el nuevo punto\n",
    "        x_new = x + a*d\n",
    "\n",
    "        # Guardar el nuevo punto y el valor de la función objetivo\n",
    "        push!(x_k, x_new)\n",
    "        push!(f_k, f(x_new))\n",
    "\n",
    "        # Calcular el error\n",
    "        error = norm(df(x))\n",
    "        push!(errors, error)\n",
    "\n",
    "        # Si se cumple el criterio de paro, terminar\n",
    "        if error < e\n",
    "            return x_new, x_k, f_k, errors, k, true\n",
    "        end\n",
    "\n",
    "        # Actualización del Hessiano Inverso\n",
    "        s = x_new - x\n",
    "        y = df(x_new) - df(x)\n",
    "        rho = 1/(y'*s)\n",
    "        H_I = (I -rho*s*y')*H_I*(I-rho*y*s') + rho*s*s'\n",
    "\n",
    "        # Actualizar el punto actual\n",
    "        x = x_new\n",
    "\n",
    "    end\n",
    "\n",
    "    return x, x_k, f_k, errors, maxIter, false\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Descenso Gradiente de Newton (Hessiano Exacto)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradNewtonExact (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function GradNewtonExact(f, df, ddf, x0, a, maxIter, e, stopCriterion)\n",
    "\n",
    "    λ = 1e-6  # Regularización\n",
    "\n",
    "    # Definir el punto inicial\n",
    "    x = x0\n",
    "\n",
    "    # x_k es la secuencia de puntos x obtenidos en cada iteración\n",
    "    x_k = [x0]\n",
    "\n",
    "    # f_k es la secuencia de valores de la función objetivo en cada iteración\n",
    "    f_k = [f(x0)]\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    for k in 1:maxIter\n",
    "        # Obtener la dirección de descenso\n",
    "        g = df(x)\n",
    "        H = ddf(x) + λ * I\n",
    "\n",
    "        # Resolver el sistema de ecuaciones\n",
    "        d = -H\\g\n",
    "\n",
    "        # Calcular el nuevo punto\n",
    "        x_new = x + a*d\n",
    "\n",
    "        # Guardar el nuevo punto y el valor de la función objetivo\n",
    "        push!(x_k, x_new)\n",
    "        push!(f_k, f(x_new))\n",
    "\n",
    "        # Calcular el error\n",
    "        error = norm(df(x))\n",
    "        push!(errors, error)\n",
    "\n",
    "        # Si se cumple el criterio de paro, terminar\n",
    "        if error < e\n",
    "            return x_new, x_k, f_k, errors, k, true\n",
    "        end\n",
    "\n",
    "        # Actualizar el punto actual\n",
    "        x = x_new\n",
    "    end\n",
    "\n",
    "    return x, x_k, f_k, errors, maxIter, false\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testeando la función $ f : \\R^{2} \\to \\R $ dada por**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x,y) = x^{4}+y^{4}-4xy+\\frac{1}{2}y + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function f(x)\n",
    "    return x[1]^4 + x[2]^4 - 4*x[1]*x[2] + (1/2)*x[2] +1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradiente de la función**:\n",
    "\n",
    "El gradiente $\\nabla f(x,y)$\n",
    "\n",
    "$$ \\nabla f(x) = \n",
    "\n",
    "\\begin{bmatrix}\n",
    "4x^{3} - 4y \\\\\n",
    "4y^{3} - 4x + \\frac{1}{2} \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function df(x)\n",
    "    return [4*x[1]^3 - 4*x[2], 4*x[2]^3 - 4*x[1] + 0.5, 0, 0]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hessiano de la función**:\n",
    "\n",
    "El hessiano $H(f(x,y))$\n",
    "\n",
    "$$ H(f) = \n",
    "\\begin{bmatrix}\n",
    "12x^{2} & -4 & 0 & 0\\\\\n",
    "-4 & 12y^{2} & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ddf (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function ddf(x)\n",
    "    return [12*x[1]^2  -4  0  0; \n",
    "            -4  12*x[2]^2  0  0; \n",
    "            0  0  0  0; \n",
    "            0  0  0  0]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Punto inicial**: $x_0 = (-3, 1, -3, 1)^{T}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " -3.0\n",
       "  1.0\n",
       " -3.0\n",
       "  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x0 = [-3.0, 1.0, -3.0, 1.0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":gradiente"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "α = 0.1  # Tamaño de paso\n",
    "maxIter = 900  # Máximo de iteraciones\n",
    "ε = 1e-6  # Tolerancia\n",
    "stop_criterion = :gradiente  # Criterio de paro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de Descenso Gradiente Naive con Dirección Aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solución óptima: [-2.7135487067672357, 2.5709529455894033, -4.473901394558214, 0.8557672204293575]\n",
      "Número de iteraciones: 900\n",
      "Valor de f en la solución: 128.0994170122159\n",
      "Convergencia: false\n"
     ]
    }
   ],
   "source": [
    "x_opt, xk, fk, errores, iteraciones, convergencia = GradNaiveRand(f, df, x0, α, maxIter, ε, stop_criterion)\n",
    "println(\"Solución óptima: \", x_opt)\n",
    "println(\"Número de iteraciones: \", iteraciones)\n",
    "println(\"Valor de f en la solución: \", f(x_opt))\n",
    "println(\"Convergencia: \", convergencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de Descenso Máximo Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solución óptima: [-Inf, Inf, -3.0, 1.0]\n",
      "Número de iteraciones: 900\n",
      "Valor de f en la solución: Inf\n",
      "Convergencia: false\n"
     ]
    }
   ],
   "source": [
    "x_opt, xk, fk, errores, iteraciones, convergencia = GradNaiveMax(f, df, x0, α, maxIter, ε, stop_criterion)\n",
    "println(\"Solución óptima: \", x_opt)\n",
    "println(\"Número de iteraciones: \", iteraciones)\n",
    "println(\"Valor de f en la solución: \", f(x_opt))\n",
    "println(\"Convergencia: \", convergencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de Descenso Gradiente de Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solución óptima: [0.9831456408735608, 0.9502844966900418, -3.0, 1.0]\n",
      "Número de iteraciones: 227\n",
      "Valor de f en la solución: -0.5121797144430129\n",
      "Convergencia: true\n"
     ]
    }
   ],
   "source": [
    "x_opt, xk, fk, errores, iteraciones, convergencia = GradNewton(f, df, x0, α, maxIter, ε, stop_criterion)\n",
    "println(\"Solución óptima: \", x_opt)\n",
    "println(\"Número de iteraciones: \", iteraciones)\n",
    "println(\"Valor de f en la solución: \", f(x_opt))\n",
    "println(\"Convergencia: \", convergencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de Descenso Gradiente de Newton (Hessiano Exacto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solución óptima: [-1.014628463704684, -1.0445304960544308, -3.0, 1.0]\n",
      "Número de iteraciones: 307\n",
      "Valor de f en la solución: -1.5113194477216192\n",
      "Convergencia: true\n"
     ]
    }
   ],
   "source": [
    "x_opt, xk, fk, errores, iteraciones, convergencia = GradNewtonExact(f, df, ddf, x0, α, maxIter, ε, stop_criterion)\n",
    "println(\"Solución óptima: \", x_opt)\n",
    "println(\"Número de iteraciones: \", iteraciones)\n",
    "println(\"Valor de f en la solución: \", f(x_opt))\n",
    "println(\"Convergencia: \", convergencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Óptimo**: $x^{*} = (-1.01463, -1.04453)^{T}, \\ \\ \\ f(x^{*}) = -1.51132$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
